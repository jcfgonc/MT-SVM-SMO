% This file was created with JabRef 2.8.1.
% Encoding: Cp1252

@ARTICLE{Ando2005,
  author = {Rie Ando and Tong Zhang},
  title = {A Framework for Learning predictive structures from multiple tasks
	and unlabeled data},
  journal = {J. of Machine Learning Research},
  year = {2005},
  volume = {6},
  pages = {1817--1853}
}

@MISC{Asuncion2010,
  author = {A. Asuncion and D. J. Newman},
  title = {{UCI} Machine Learning Repository},
  year = {2010},
  organization = {University of California, Irvine, School of Information and Computer
	Sciences},
  owner = {João Carlos},
  timestamp = {2012.02.01},
  url = {http://archive.ics.uci.edu/ml/index.html}
}

@INPROCEEDINGS{Ayat02kmod,
  author = {N.E. Ayat and M. Cheriet and C.Y. Suen and M. Cheriet C. Y. Suen},
  title = {{KMOD} - A Two-parameter {SVM} Kernel for Pattern Recognition},
  booktitle = {ICPR},
  year = {2002},
  pages = {30331--30334}
}

@BOOK{Bishop2006,
  title = {Pattern Recognition and Machine Learning (Information Science and
	Statistics)},
  publisher = { Springer-Verlag},
  year = {2006},
  author = {Christopher M. Bishop},
  address = { New York, Inc. Secaucus, NJ, USA}
}

@ARTICLE{Blythe2008,
  author = {David Blythe},
  title = {Rise of the Graphics Processor},
  journal = {Vol. 96, No. 5, May 2008 | Proceedings of the IEEE},
  year = {2008},
  volume = {96},
  pages = {761-778},
  abstract = {The modern graphics processing unit (GPU) is the result of 40 years
	of evolution of hardware to accelerate graphics processing operations.
	It represents the convergence of support for multiple market segments:
	computer-aided design, medical imaging, digital content creation,
	document and presentation applications, and entertainment applications.
	The exceptional performance characteristics of the GPU make it an
	attractive target for other application domains. We examine some
	of this evolution, look at the structure of a modern GPU, and discuss
	how graphics processing exploits this structure and how nongraphical
	applications can take advantage of this capability. We discuss some
	of the technical and market issues around broader adoption of this
	technology.},
  owner = {João Carlos},
  timestamp = {2012.02.21}
}

@ARTICLE{Brereton2010,
  author = {Brereton, Richard G. and Lloyd, Gavin R.},
  title = {Support Vector Machines for classification and regression},
  journal = {Analyst},
  year = {2010},
  volume = {135},
  pages = {230-267},
  abstract = {The increasing interest in Support Vector Machines (SVMs) over the
	past 15 years is described. Methods are illustrated using simulated
	case studies{,} and 4 experimental case studies{,} namely mass spectrometry
	for studying pollution{,} near infrared analysis of food{,} thermal
	analysis of polymers and UV/visible spectroscopy of polyaromatic
	hydrocarbons. The basis of SVMs as two-class classifiers is shown
	with extensive visualisation{,} including learning machines{,} kernels
	and penalty functions. The influence of the penalty error and radial
	basis function radius on the model is illustrated. Multiclass implementations
	including one vs. all{,} one vs. one{,} fuzzy rules and Directed
	Acyclic Graph (DAG) trees are described. One-class Support Vector
	Domain Description (SVDD) is described and contrasted to conventional
	two- or multi-class classifiers. The use of Support Vector Regression
	(SVR) is illustrated including its application to multivariate calibration{,}
	and why it is useful when there are outliers and non-linearities.},
  doi = {10.1039/B918972F},
  issue = {2},
  publisher = {The Royal Society of Chemistry},
  url = {http://dx.doi.org/10.1039/B918972F}
}

@ELECTRONIC{Burnett2006,
  author = {Colin M.L. Burnett},
  month = {December},
  year = {2006},
  title = {A generic 4-stage pipeline},
  howpublished = {Internet},
  organization = {Wikipedia},
  url = {http://en.wikipedia.org/wiki/File:Pipeline,_4_stage.svg},
  owner = {João Carlos},
  timestamp = {2012.02.01}
}

@ARTICLE{L.J.Cao2006,
  author = {L. Cao and S. Keerthi and Chong-Jin Ong and J. Zhang and U. Periyathamby,
	Xiu Ju Fu and H. Lee},
  title = {Parallel Sequential Minimal Optimization for the Training of Support
	Vector Machines},
  journal = {IEEE Transactions on neural networks},
  year = {2006},
  volume = {17},
  pages = {1039--1049},
  number = {4},
  abstract = {Sequential minimal optimization (SMO) is one pop-ular algorithm for
	training support vector machine (SVM), but it still requires a large
	amount of computation time for solving large size problems. This
	paper proposes one parallel implementation of SMO for training SVM.
	The parallel SMO is developed using mes-sage passing interface (MPI).
	Specifically, the parallel SMO first partitions the entire training
	data set into smaller subsets and then simultaneously runs multiple
	CPU processors to deal with each of the partitioned data sets. Experiments
	show that there is great speedup on the adult data set and the Mixing
	National Institute of Standard and Technology (MNIST) data set when
	many proces-sors are used. There are also satisfactory results on
	the Web data set.}
}

@INPROCEEDINGS{Catanzaro2008,
  author = {Catanzaro, Bryan and Sundaram, Narayanan and Keutzer, Kurt},
  title = {Fast support vector machine training and classification on graphics
	processors},
  booktitle = {Proc. of the 25th Int Conf on Machine learning},
  year = {2008},
  series = {ICML '08},
  pages = {104-111},
  address = {NY, USA},
  publisher = {ACM}
}

@MANUAL{Chang2011,
  title = {LIBSVM: A Library for Support Vector Machines},
  author = {Chih-Chung Chang and Chih-Jen Lin},
  organization = {Dep. of Computer Science National Taiwan University, Taipei, Taiwan},
  year = {2011},
  abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been
	actively developing this package since the year 2000. The goal is
	to help users to easily apply SVM to their applications. LIBSVM has
	gained wide popu-larity in machine learning and many other areas.
	In this article, we present all implementation details of LIBSVM.
	Issues such as solving SVM optimiza-tion problems, theoretical convergence,
	multi-class classification, probability estimates, and parameter
	selection are discussed in detail.},
  owner = {João Carlos},
  timestamp = {2012.01.24}
}

@ARTICLE{Chen2012,
  author = {Chen, Badong and Zhao, Songlin and Zhu, Pingping and Príncipe, José
	Carlos},
  title = {Quantized Kernel Least Mean Square Algorithm.},
  journal = {IEEE Trans. Neural Netw. Learning Syst.},
  year = {2012},
  volume = {23},
  pages = {22--32},
  number = {1}
}

@BOOK{Cherkassky2007,
  title = {Learning from Data},
  publisher = {John Wiley \& Sons, New York},
  year = {2007},
  author = {Cherkassky, V. and Mulier, F.}
}

@INPROCEEDINGS{CorVap95,
  author = {Corinna Cortes and Vladimir Vapnik},
  title = {Support-Vector Networks},
  booktitle = {Machine Learning},
  year = {1995},
  pages = {273--297},
  abstract = {The support-vector network is a new learning machine for two-group
	classification problems. The machine conceptually implements the
	following idea: input vectors are non-linearly mapped to a very high-dimension
	feature space. In this feature space a linear decision surface is
	constructed. Special properties of the decision surface ensures high
	generalization ability of the learning machine. The idea behind the
	supportvector network was previously implemented for the restricted
	case where the training data can be separated without errors. We
	here extend this result to non-separable training data}
}

@ARTICLE{Suykens2010,
  author = {De Brabanter, K. and De Brabanter, J. and Suykens, J. and De Moor,
	B.},
  title = {Optimized fixed-size kernel models for large data sets},
  journal = {Comput. Stat. Data Anal.},
  year = {2010},
  volume = {54},
  pages = {1484--1504},
  number = {6},
  address = {Amsterdam, The Netherlands, The Netherlands},
  publisher = {Elsevier Science Publishers B. V.}
}

@ARTICLE{Suykens2006,
  author = {Espinoza, Marcelo and Suykens, Johan and Moor, Bart},
  title = {Fixed-size Least Squares Support Vector Machines: A Large Scale Application
	in Electrical Load Forecasting},
  journal = {Computational Management Science},
  year = {2006},
  volume = {3},
  pages = {113-129},
  note = {10.1007/s10287-005-0003-7},
  affiliation = {Katholieke Universiteit Leuven ESAT/SISTA Kasteelpark Arenberg 10
	3000 Leuven Belgium},
  issn = {1619-697X},
  issue = {2},
  keyword = {Business and Economics},
  publisher = {Springer Berlin / Heidelberg},
  url = {http://dx.doi.org/10.1007/s10287-005-0003-7}
}

@ARTICLE{Fan2005,
  author = {Rong-En Fan and Pai-Hsuen Chen and Chih-Jen Lin},
  title = {Working Set Selection Using Second Order Information for Training
	Support Vector Machines},
  journal = {J. of Machine Learning Research},
  year = {2005},
  volume = {6},
  pages = {1889-1918},
  month = {December},
  owner = {João Carlos},
  timestamp = {2012.01.24},
  url = {http://dl.acm.org/citation.cfm?id=1046920.1194907}
}

@ARTICLE{Fawcett2006,
  author = {T. E. Fawcett},
  title = {An introduction to {ROC} analysis},
  journal = {Pattern Recognition Letters},
  year = {2006},
  volume = {27},
  pages = {861--874}
}

@ARTICLE{Ferrer2005,
  author = {Ferrer, M. and Alonso, J. and Travieso, C.},
  title = {Offline geometric parameters for automatic signature verification
	using fixed-point arithmetic},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2005},
  volume = {27},
  pages = {993--997},
  number = {6},
  abstract = {{This paper presents a set of geometric signature features for offline
	automatic signature verification based on the description of the
	signature envelope and the interior stroke distribution in polar
	and Cartesian coordinates. The features have been calculated using
	16 bits fixed-point arithmetic and tested with different classifiers,
	such as hidden Markov models, support vector machines, and Euclidean
	distance classifier. The experiments have shown promising results
	in the task of discriminating random and simple forgeries.}},
  owner = {João Carlos},
  timestamp = {2012.02.21}
}

@ELECTRONIC{Gutierrez-Osuna2011,
  author = {Ricardo Gutierrez-Osuna},
  month = {November},
  year = {2011},
  title = {Pattern Analysis - Lesson 10: Linear discriminants analysis},
  language = {English},
  organization = {Texas A\&M University},
  url = {http://research.cs.tamu.edu/prism/lectures/pr/pr_l10.pdf},
  owner = {ck},
  timestamp = {2012.08.29}
}

@ELECTRONIC{Harris2007,
  author = {Mark Harris},
  year = {2007},
  title = {Optimizing Parallel Reduction in CUDA},
  language = {English},
  howpublished = {Internet},
  organization = {NVIDIA},
  url = {http://developer.download.nvidia.com/compute/cuda/1_1/Website/projects/reduction/doc/reduction.pdf},
  abstract = {A parallel sum reduction that computes the sum of large arrays of
	values. This sample demonstrates several important optimization stratezies
	for parallel algorithms like reduction.},
  owner = {João Carlos},
  timestamp = {2012.02.02}
}

@INPROCEEDINGS{Herrero2010,
  author = {Sergio Herrero-Lopez and John R. Williams and Abel Sanchez},
  title = {Parallel multiclass classification using {SVM}s on {GPU}s.},
  booktitle = {GPGPU 10},
  year = {2010},
  pages = {2--11}
}

@ARTICLE{Suykens2005,
  author = {Hoegaerts, L. and Suykens, J. A. K. and Vandewalle, J. and De Moor,
	B.},
  title = {Subset based least squares subspace regression in {RKHS}},
  journal = {Neurocomput.},
  year = {2005},
  volume = {63},
  pages = {293--323},
  acmid = {1745572},
  address = {Amsterdam, The Netherlands, The Netherlands},
  doi = {10.1016/j.neucom.2004.04.013},
  issn = {0925-2312},
  issue_date = {January, 2005},
  keywords = {Eigenspaces, Fixed-size least squares support vector machine, Kernel
	canonical correlation analysis, Kernel methods, Kernel partial least
	squares, Kernel principal component analysis, Nystr\"{o}m approximation,
	Reduced set methods, Regression, Reproducing kernel Hilbert space,
	Subspaces},
  numpages = {31},
  publisher = {Elsevier Science Publishers B. V.}
}

@ARTICLE{Keerthi2001,
  author = {Keerthi, S. and Shevade, S. and Bhattacharyya, C. and Murthy, K.},
  title = {Improvements to {P}latt's {SMO} Algorithm for {SVM} Classifier Design},
  journal = {Neural Comput.},
  year = {2001},
  volume = {13},
  pages = {637--649},
  number = {3},
  publisher = {MIT Press}
}

@INPROCEEDINGS{Lin2010,
  author = {Tsung-Kai Lin and Shao-Yi Chien},
  title = {Support Vector Machines on GPU with Sparse Matrix Format},
  booktitle = {Proceedings Ninth International Conference on Machine Learning and
	Applications (ICMLA 2010)},
  year = {2010},
  pages = {313--318},
  publisher = {IEEE Computer Society}
}

@INPROCEEDINGS{cit:Lopes2012,
  author = {Noel Lopes and Daniel Correia and Carlos Pereira and Bernardete Ribeiro
	and Ant{'{o}}nio Dourado},
  title = {An Incremental Hypersphere Learning Framework for Protein Membership
	Prediction},
  booktitle = {Int. Conf. on Hybrid Artificial Intelligence Systems},
  year = {2012},
  series = {LNCS 7208},
  pages = {429--439},
  owner = {Mila},
  timestamp = {2012.08.13}
}

@ARTICLE{LopesRibeiro2011,
  author = {Noel Lopes and Bernardete Ribeiro},
  title = {{GPUMLib}: An Efficient Open-Source {GPU} Machine Learning Library},
  journal = {Int J of Computer Information Systems and Industrial Management Applications},
  year = {2011},
  volume = {3},
  pages = {355--362},
  owner = {João Carlos},
  timestamp = {2012.02.21}
}

@INPROCEEDINGS{LopesRibeiroQuintas2010,
  author = {Lopes, N. and Ribeiro, B. and Quintas, R.},
  title = {{GPUMLib}: A new Library to combine Machine Learning algorithms with
	Graphics Processing Units},
  booktitle = {10th Int Conf on Hybrid Intelligent Systems on},
  year = {2010},
  pages = {229--232},
  month = {August},
  doi = {10.1109/HIS.2010.5600028},
  keywords = {GPU machine learning library;GPUMLib;ML algorithm;benchmark datasets;computational
	power;data parallel computation;graphics processing unit;hybrid intelligent
	real world applications;many core device;coprocessors;graphical user
	interfaces;learning (artificial intelligence);},
  owner = {João Carlos},
  timestamp = {2012.02.21}
}

@BOOK{mitchell97,
  title = {Machine Learning},
  publisher = {McGraw-Hill},
  year = {1997},
  author = {Mitchell, Tom},
  address = {Columbus, OH},
  key = {Mitchell}
}

@ARTICLE{Nickolls2010,
  author = {Nickolls, J.; Dally, W.J.;},
  title = {The GPU Computing Era},
  journal = {IEEE Micro},
  year = {2010},
  volume = {30},
  pages = {56 - 69},
  owner = {Joana Carla},
  timestamp = {2012.02.21}
}

@MANUAL{CudaProgGuide,
  title = {CUDA Programming Guide Version 4.0},
  author = {NVIDIA},
  month = {August},
  year = {2011},
  owner = {João Carlos},
  timestamp = {2012.02.21}
}

@ELECTRONIC{NVIDIA2006,
  author = {NVIDIA},
  month = {November},
  year = {2006},
  title = {NVIDIA GeForce 8800 GPU Architecture Overview},
  howpublished = {Internet},
  organization = {NVIDIA},
  owner = {Joana Carla},
  timestamp = {2012.02.21}
}

@ELECTRONIC{Nvidiapixshaders,
  author = {NVIDIA},
  month = {March},
  year = {2001},
  title = {Pixel Shaders - A Facet of the nfiniteFX Engine},
  language = {English},
  howpublished = {Internet},
  url = {http://www.nvidia.com/object/feature_pixelshader.html},
  owner = {João Carlos},
  timestamp = {2011.09.28}
}

@ELECTRONIC{nvidiavertexshaders,
  author = {NVIDIA},
  month = {March},
  year = {2001},
  title = {Vertex Shaders - A Facet of the nfiniteFX Engine},
  language = {English},
  howpublished = {Internet},
  url = {http://www.nvidia.com/object/feature_vertexshader.html},
  owner = {João Carlos},
  timestamp = {2011.09.28}
}

@ELECTRONIC{Nvidia1999,
  author = {NVIDIA},
  month = {August},
  year = {1999},
  title = {GeForce 256 - The World's First GPU},
  howpublished = {Internet},
  url = {http://www.nvidia.com/page/geforce256.html},
  abstract = {August 31, 1999 marked the introduction of the graphics processing
	unit (GPU) for the PC industry the NVIDIA GeForce 256. The technical
	definition of a GPU is "a single-chip processor with integrated transform,
	lighting, triangle setup/clipping, and rendering engines that is
	capable of processing a minimum of 10 million polygons per second."
	With transform, lighting, setup, and rendering on a single GPU, the
	GeForce 256 delivers 15M polygons/second and 480M pixels/second of
	performance. Truly revolutionary, its unique 256-bit rendering engine
	enabled an order of magnitude increase in visual complexity, and
	helped to set the stage for the future of realism in graphics.},
  owner = {João Carlos},
  timestamp = {2011.09.15}
}

@INPROCEEDINGS{Osuna1997,
  author = {Osuna, E. and Freund, R. and Girosi, F.},
  title = {An improved training algorithm for support vector machines},
  booktitle = {Proceedings IEEE Neural Networks in Signal Processing NNSP'97},
  year = {1997},
  pages = {276-285},
  publisher = {IEEE Computer Society}
}

@ARTICLE{Owens2008,
  author = {John D. Owens and Mike Houston and David Luebke and Simon Green and
	John Stone and James Phillips},
  title = {{GPU} Computing},
  journal = {Vol. 96, No. 5, May 2008 | Proceedings of the IEEE},
  year = {2008},
  volume = {96},
  pages = {879 - 899},
  abstract = {The graphics processing unit (GPU) has become an integral part of
	today's mainstream computing systems. Over the past six years, there
	has been a marked increase in the performance and capabilities of
	GPUs. The modern GPU is not only a powerful graphics engine but also
	a highly parallel programmable processor featuring peak arithmetic
	and memory bandwidth that substantially outpaces its CPU counterpart.
	The GPUs rapid increase in both programmability and capability has
	spawned a research community that has successfully mapped a broad
	range of computationally demanding, complex problems to the GPU.
	This effort in generalpurpose computing on the GPU, also known as
	GPU computing, has positioned the GPU as a compelling alternative
	to traditional microprocessors in high-performance computer systems
	of the future. We describe the background, hardware, and programming
	model for GPU computing, summarize the state of the art in tools
	and techniques, and present four GPU computing successes in game
	physics and computational biophysics that deliver order-of-magnitude
	performance gains over optimized CPU applications.},
  owner = {João Carlos},
  timestamp = {2012.02.21}
}

@INBOOK{Platt1998,
  pages = {1--21},
  title = {Sequential minimal optimization: A fast algorithm for training support
	vector machines},
  publisher = {MIT Press},
  year = {1998},
  editor = {Sch\"{o}lkopf, Bernhard and Burges, Christopher J. C. and Smola,
	Alexander J.},
  author = {John Platt},
  volume = {208},
  journal = {Advances in Kernel Methods - Support Vector Learning}
}

@INPROCEEDINGS{Qiao2009,
  author = {Qiao, Mengyu and Sung, Andrew H. and Liu, Qingzhong},
  title = {Feature Mining and Intelligent Computing for {MP3} Steganalysis},
  booktitle = {Int Joint Conf on Bioinformatics, Systems Biology and Intelligent
	Computing},
  year = {2009},
  pages = {627--630},
  publisher = {IEEE Computer Society}
}

@ELECTRONIC{Rege2008,
  author = {Ashu Rege},
  year = {2008},
  title = {An Introduction to Modern GPU Architecture},
  organization = {NVIDIA},
  owner = {Joana Carla},
  timestamp = {2012.02.21}
}

@BOOK{Marques2001,
  title = {Pattern recognition: concepts, methods, and applications},
  publisher = {Springer},
  year = {2001},
  author = {Joaquim Marques de Sá}
}

@ARTICLE{SchMikBurKniMueRaeSmo99,
  author = {B.~Sch{\"o}lkopf and S.~Mika and C.J.C.~Burges and P.~Knirsch and
	K.--R.~M{\"u}ller and G.~R{\"a}tsch and A.~Smola},
  title = {Input Space vs.~Feature Space in Kernel-Based Methods},
  journal = {IEEE Transactions on Neural Networks},
  year = {1999},
  volume = {10},
  pages = {1000-1017},
  note = {IEEE Transactions on Neural Networks}
}

@ELECTRONIC{Schneider1997,
  author = {Jeff Schneider},
  month = {February},
  year = {1997},
  title = {Cross Validation},
  url = {http://www.cs.cmu.edu/~schneide/tut5/node42.html},
  owner = {Joana Carla},
  timestamp = {2012.03.02}
}

@ELECTRONIC{Smith2010,
  author = {Ryan Smith},
  month = {2010},
  year = {2010},
  title = {NVIDIA's GF100: Architected for Gaming},
  howpublished = {AnandTech},
  url = {http://www.anandtech.com/show/2918},
  abstract = {At this year's Consumer Electronics Show, NVIDIA had several things
	going on. In a public press conference they announced 3D Vision Surround
	and Tegra 2, while on the showfloor they had products oplenty, including
	a GF100 setup showcasing 3D Vision Surround But if youre here, then
	what youre most interested in is what wasnt talked about in public,
	and that was GF100. With the Fermi-based GF100 GPU finally in full
	production, NVIDIA was ready to talk to the press about the rest
	of GF100, and at the tail-end of CES we got our first look at GF100s
	gaming abilities, along with a hands-on look at some unknown GF100
	products in action. The message NVIDIA was trying to send: GF100
	is going to be here soon, and its going to be fast.},
  owner = {João Carlos},
  timestamp = {2011.09.15}
}

@ELECTRONIC{Smith2010a,
  author = {Ryan Smith},
  month = {11},
  year = {2010},
  title = {NVIDIAs GeForce GTX 460: The 200 King},
  howpublished = {AnandTech},
  url = {http://www.anandtech.com/show/3809/nvidias-geforce-gtx-460-the-200-king},
  owner = {João Carlos},
  timestamp = {2011.09.15}
}

@BOOK{Smola2000,
  title = {Advances in Large Margin Classifiers},
  publisher = {The MIT Press},
  year = {2000},
  author = {Alexander J. Smola and Peter Bartlett and Bernhard Sch\"{o}lkopf
	and Dale Schuurmans (Eds.)},
  edition = {First},
  month = {October}
}

@MANUAL{Spital1987,
  title = {Sinclair ZX Spectrum +2A 128K Manual},
  author = {Ivor Spital},
  organization = {AMSTRAD Plc},
  year = {1987},
  owner = {João Carlos},
  timestamp = {2012.02.21}
}

@BOOK{Konstantinos2009,
  title = {Pattern recognition},
  publisher = { Academic Press},
  year = {2009},
  author = {Sergios Theodoridis and Konstantinos Koutroumbas}
}

@BOOK{Vapnik2006,
  title = {Empirical inference science afterword of 2006},
  publisher = {Springer},
  year = {2006},
  author = {Vladimir Vapnik}
}

@BOOK{Vapnik1998,
  title = {Statistical Learning Theory},
  publisher = {Wiley New York, Inc.},
  year = {1998},
  author = {Vladimir Vapnik}
}

@BOOK{Vapnik1995,
  title = {The nature of statistical learning theory},
  publisher = {Springer-Verlag},
  year = {1995},
  author = {Vladimir Vapnik}
}

@BOOK{Vapnik1982,
  title = {Estimation of Dependences Based on Empirical Data},
  publisher = {Springer-Verlag, New York, Inc.},
  year = {1982},
  author = {Vladimir Vapnik}
}

@ARTICLE{Vapnik2009,
  author = {Vapnik, Vladimir and Vashist, Akshay},
  title = {A new learning paradigm: Learning using privileged information},
  journal = {Neural Networks},
  year = {2009},
  volume = {22},
  pages = {544--557},
  number = {5-6},
  month = {July},
  abstract = {In contrast to the existing machine learning paradigm where a teacher
	does not play an important role, the advanced learning paradigm considers
	some elements of human teaching. In the new paradigm along with examples,
	a teacher can provide students with hidden information that exists
	in explanations, comments, comparisons, and so on. This paper discusses
	details of the new paradigm 1 and corresponding algorithms, introduces
	some new algorithms, considers several specific forms of privileged
	information, demonstrates superiority of the new learning paradigm
	over the classical learning paradigm when solving practical problems,
	and discusses general questions related to the new ideas.},
  doi = {10.1016/j.neunet.2009.06.042},
  url = {http://dx.doi.org/10.1016/j.neunet.2009.06.042}
}

@ARTICLE{Wu2010,
  author = {Wei Wen Wu},
  title = {Beyond business failure prediction},
  journal = {Expert Systems with Applications},
  year = {2010},
  volume = {37},
  pages = {2371--2376}
}

@ARTICLE{Rui2011,
  author = {Rui Zhang and Wenjian Wang},
  title = {Facilitating the applications of support vector machine by using
	a new kernel},
  journal = {Expert Systems with Applications},
  year = {2011},
  volume = {38},
  pages = {14225--14230}
}

@ARTICLE{ZieRaeMikSchLenMue00,
  author = {A.~Zien and G.~R{\"a}tsch and S.~Mika and B.~Sch{\"o}lkopf, C.~Lemmen
	and A.~Smola and T.~Lengauer and K.-R.~Mueller},
  title = {Engineering Support Vector Machine Kernel That Recognize Translation
	Initiation Sites},
  journal = {Bioinformatics},
  year = {2000},
  volume = {16},
  pages = {799--807},
  number = {9},
  publisher = {Oxford University Press}
}

